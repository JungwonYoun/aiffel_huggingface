{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4144747d",
   "metadata": {},
   "source": [
    "# 프로젝트 : 커스텀 프로젝트 직접 만들기\n",
    "\n",
    "KLUE의 model(klue/ber-base)를 활용하여 NSMC(Naver Sentiment Movie Corpus) task를 도전해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bbbc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11152365",
   "metadata": {},
   "source": [
    "## STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "\n",
    "* 데이터셋은 깃허브에서 다운받거나, Huggingface datasets에서 가져올 수 있습니다. 앞에서 배운 방법들을 활용해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f3d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9257ccbd54e645628c183794fb434e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8804aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5743ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c3dac",
   "metadata": {},
   "source": [
    "## 📊 분석\n",
    "huggingface nsmc dataset을 확인해보면 위와 같이 구성되어 있습니다.\n",
    "\n",
    "Dataset dictionary안에 train dataset, test dataset으로 구성되어 있고 각 Dataset은 ‘id’, ‘documnet’, ‘label’로 구성되어 있습니다.\n",
    "\n",
    "\n",
    "* This is a movie review dataset in the Korean language. Reviews were scraped from Naver Movies.\n",
    "\n",
    "    Each file is consisted of three columns: id, document, label\n",
    "    * id: The review id, provieded by Naver\n",
    "    * document: The actual review\n",
    "    * label: The sentiment class of the review. (0: negative, 1: positive)\n",
    "\n",
    "**Characteristics**\n",
    "    \n",
    "    All reviews are shorter than 140 characters\n",
    "    Each sentiment class is sampled equally (i.e., random guess yields 50% accuracy)\n",
    "    100K negative reviews (originally reviews of ratings 1-4)\n",
    "    100K positive reviews (originally reviews of ratings 9-10)\n",
    "    Neutral reviews (originally reviews of ratings 5-8) are excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc99c81",
   "metadata": {},
   "source": [
    "## STEP 2. klue/bert-base model 및 tokenizer 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ba4c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)\n",
    "config = huggingface_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bd108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a5f348",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train & validation & test split\n",
    "\n",
    "# hf_train_dataset = load_dataset('nsmc', split = 'train[:80%]')\n",
    "# hf_val_dataset = load_dataset('nsmc', split = 'train[80%:100%]')\n",
    "# hf_test_dataset = load_dataset('nsmc', split = 'test')\n",
    "\n",
    "# hf_train_dataset = hf_train_dataset.map(transform, batched=True)\n",
    "# hf_train_dataset\n",
    "\n",
    "# hf_val_dataset = hf_val_dataset.map(transform, batched=True)\n",
    "# hf_val_dataset\n",
    "\n",
    "# hf_test_dataset = hf_test_dataset.map(transform, batched=True)\n",
    "# hf_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6b99c",
   "metadata": {},
   "source": [
    "## 🚨 수정\n",
    "### test데이터 수를 너무 늘리면 모델 run 시간이 너무 오래걸리므로, 주어진 1~2%의 데이터만 사용하여 돌리는 것으로 수정!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2029fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n",
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n",
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-c28c09024d5ef772.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-4f10c8a4eb4783af.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-71131dd861084182.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
      "    num_rows: 3000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
      "    num_rows: 1500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'document', 'id', 'input_ids', 'label'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 2차 train & validation & test split\n",
    "\n",
    "hf_train_dataset = load_dataset('nsmc', split = 'train[:2%]')\n",
    "hf_val_dataset = load_dataset('nsmc', split = 'train[2%:3%]')\n",
    "hf_test_dataset = load_dataset('nsmc', split = 'test[:2%]')\n",
    "\n",
    "hf_train_dataset = hf_train_dataset.map(transform, batched=True)\n",
    "\n",
    "\n",
    "hf_val_dataset = hf_val_dataset.map(transform, batched=True)\n",
    "\n",
    "\n",
    "hf_test_dataset = hf_test_dataset.map(transform, batched=True)\n",
    "\n",
    "print(hf_train_dataset)\n",
    "print(hf_val_dataset)\n",
    "print(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31275d5",
   "metadata": {},
   "source": [
    "## STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c24c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8e9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6dbd5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 17:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.405592</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.857898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.495182</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>0.858141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.198200</td>\n",
       "      <td>0.611858</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>0.864214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=0.2702951422797309, metrics={'train_runtime': 1030.738, 'train_samples_per_second': 8.732, 'train_steps_per_second': 1.091, 'total_flos': 2367999498240000.0, 'train_loss': 0.2702951422797309, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcde6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6369144916534424,\n",
       " 'eval_accuracy': 0.859,\n",
       " 'eval_f1': 0.8648130393096836,\n",
       " 'eval_runtime': 32.1174,\n",
       " 'eval_samples_per_second': 31.136,\n",
       " 'eval_steps_per_second': 3.892,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041637b5",
   "metadata": {},
   "source": [
    "## STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교\n",
    "* 아래 링크를 바탕으로 bucketing과 dynamic padding이 무엇인지 알아보고, 이들을 적용하여 model을 학습시킵니다.\n",
    "* STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f0d1c",
   "metadata": {},
   "source": [
    "## Bucketing이란?\n",
    "\n",
    "\n",
    "다른 많은 데이터와 마찬가지로 순차 데이터도 가능하다면 매 epoch 마다 순서를 섞어 미니 배치를 만드는 것이 좋습니다. 그런데 데이터의 문장마다 길이는 천차만별이고, 최악의 경우 엄청 긴 하나와 엄청 짧은 나머지들이 합쳐져 미니 배치가 만들어지면 가장 긴 하나에 맞게 패딩이 이루어질 테니 엄청난 비효율이 발생합니다. 직접 C++로 짜면 모를까, 3차원 텐서를 입력으로 받는 TF의 경우 패딩을 안 하기도 힘듭니다. 이런 비효율을 그래도 최대한 방지하고자 하는 노력이 버켓(bucket)에 넣는 방법입니다.\n",
    "\n",
    "개념은 간단한데, 우선 데이터들을 스텝 길이에 따라 정렬한 뒤 몇 개의 그룹으로 나누고, 데이터를 섞더라도 그 그룹 안에서만 섞도록 하는 것입니다. 20 단어 이하, 21개 이상 40개 이하, 41개 이상 등으로 그룹을 분리할 수 있을 것입니다. 이 때는 길이가 제한되어 있으니 어느 정도 비효율적인 더미 배치가 되지는 않을 것입니다. 만약 외부에서 sequence length 를 지정해 주는 형태로 프로그래밍 했다면 각 버켓의 길이를 넣어 주면 되니 매번 동적으로 바꾸지 않아도 된다는 장점도 있습니다.\n",
    "\n",
    "\n",
    "## Dynamic padding이란?\n",
    "\n",
    "동적 패딩(Dynamic padding)\n",
    "\n",
    "전체 데이터셋이 아닌 개별 배치(batch)에 대해서 별도로 패딩(padding)을 수행하여 과도한 패딩 작업을 해주는 것을 동적 패딩이라고 합니다. \n",
    "각 배치의 가장 긴 시퀀스에 맞춰 패딩을 진행합니다. 이는 각 배치에 필요한 만큼의 패딩만 가능하게 하여 모델의 효율성을 높여줍니다.\n",
    "\n",
    "이를 수행하려면 batch로 분리하려는 데이터셋의 요소 각각에 대해서 정확한 수의 padding을 적용할 수 있도록 도와주는 collate function이 필요합니다. Transformers는 라이브러리 DataCollatorWithPadding을 통해 이러한 기능을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d3d4e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 17:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744031</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>0.862745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.892822</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.860656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.930898</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.865410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1125, training_loss=0.056650420082939995, metrics={'train_runtime': 1028.7269, 'train_samples_per_second': 8.749, 'train_steps_per_second': 1.094, 'total_flos': 2367999498240000.0, 'train_loss': 0.056650420082939995, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "collate_function = DataCollatorWithPadding(tokenizer = huggingface_tokenizer)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,# TrainingArguments을 통해 설정한 arguments\n",
    "    data_collator = collate_function, ## ❗️dynamic padding을 위해서 추가❗️\n",
    "    train_dataset=hf_train_dataset,    # training dataset\n",
    "    eval_dataset=hf_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6719f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9647986888885498,\n",
       " 'eval_accuracy': 0.864,\n",
       " 'eval_f1': 0.8671875,\n",
       " 'eval_runtime': 34.761,\n",
       " 'eval_samples_per_second': 28.768,\n",
       " 'eval_steps_per_second': 3.596,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(hf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda7704",
   "metadata": {},
   "source": [
    "## ✅ Bucketing 적용 전후 비교\n",
    "\n",
    "우선, 주어진 데이터의 1~2%만 사용하여 bucketing을 적용한 모델과 아닌 모델 모두 돌려보았다. 그 결과는 다음과 같다.\n",
    "\n",
    "* **Bucketing 적용 전**\n",
    "    \n",
    "    'train_runtime': 591.4823, \n",
    "    \n",
    "    'train_samples_per_second': 7.608, \n",
    "    \n",
    "    'train_steps_per_second': 0.954, \n",
    "    \n",
    "    'total_flos': 1183999749120000.0, '\n",
    "    \n",
    "    train_loss': 0.027098344995620402, \n",
    "    \n",
    "    'epoch': 3.0\n",
    "    \n",
    "\n",
    "    {'eval_loss': 0.5826172232627869,\n",
    "     'eval_accuracy': 0.854,\n",
    "     'eval_f1': 0.8596153846153847,\n",
    "     'eval_runtime': 34.6742,\n",
    "     'eval_samples_per_second': 28.84,\n",
    "     'eval_steps_per_second': 3.605,\n",
    "     'epoch': 3.0}\n",
    " \n",
    "* **Bucketing 적용 후**\n",
    "\n",
    "    'train_runtime': 588.6736,\n",
    "    \n",
    "    'train_samples_per_second': 7.644, \n",
    "    \n",
    "    'train_steps_per_second': 0.958, \n",
    "    \n",
    "    'total_flos': 1183999749120000.0, \n",
    "    \n",
    "    'train_loss': 0.06299833033947234, \n",
    "    \n",
    "    'epoch': 3.0\n",
    "    \n",
    "    \n",
    "    {'eval_loss': 0.9059286713600159,\n",
    "     'eval_accuracy': 0.862,\n",
    "     'eval_f1': 0.8667953667953667,\n",
    "     'eval_runtime': 32.0769,\n",
    "     'eval_samples_per_second': 31.175,\n",
    "     'eval_steps_per_second': 3.897,\n",
    "     'epoch': 3.0}\n",
    "     \n",
    "### 📊 분석\n",
    "    accuaracy 점수는 0.849에서 0.862로 증가하였고, f1 score 또한 0.859에서 0.866으로 증가하였다.\n",
    "    \n",
    "    runtime은 34.6에서 32.0으로 감소하였다.\n",
    "    \n",
    "    다만, loss값이 0.58에서 0.9로 증가하였다. \n",
    "    \n",
    "    If the loss increases and the accuracy increase too is because your regularization techniques are working well and you're fighting the overfitting problem. This is true only if the loss, then, starts to decrease whilst the accuracy continues to increase. Otherwise, if the loss keep growing your model is diverging and you should look for the cause (usually you're using a too high learning rate value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2453c",
   "metadata": {},
   "source": [
    "## 회고\n",
    "잘한 점 : Huggingface를 활용하는 코드의 방식과 흐름을 더 잘 이해할 수 있는 프로젝트였다.\n",
    "\n",
    "못한 점 : 90% 이상의 정확도를 내지 못해 아쉽다.\n",
    "\n",
    "노력할 점 : 데이터를 모두 사용하거나 parameter를 잘 조정하여 정확도를 더 높이는 방법을 찾아야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937fc6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
